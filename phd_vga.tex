\documentclass[a4paper, 11pt, openright, twoside]{reportPhD}

% Paquetes
\usepackage[T1]{fontenc}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf}
\usepackage{color}
\usepackage{amsmath}
\usepackage{txfonts}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{graphicx}
%\usepackage{subcaption}
\usepackage{subfigure}
\usepackage{float}
\usepackage{multirow}
\usepackage{lmodern}
\usepackage[final]{microtype}
\usepackage[utf8]{inputenc}
\usepackage{array,mathtools}
\usepackage[bookmarks]{hyperref}
\usepackage{xcolor}
\usepackage[inner=3cm, outer=2cm, bottom=3cm]{geometry}
%\usepackage[ruled]{algorithm2e}
\usepackage{enumerate}
\usepackage[inline,shortlabels]{enumitem}
\usepackage{xfrac}
\usepackage{booktabs}
\usepackage{multirow}

\usepackage[toc,page]{appendix}
\usepackage{courier}
\usepackage{listings}
\usepackage{lipsum}

%\usepackage[linesnumbered]{algorithm2e}

%\usepackage{calc}


\usepackage{algpseudocode}
\usepackage{algorithm}



%\newtheorem{definition}{Definition}
%\newtheorem{lemma}{Lemma}

\usepackage{color,soul}
\usepackage[usestackEOL]{stackengine}[2013-09-11]
\def\stackalignment{l}
\setstackgap{L}{1.4\baselineskip}
\fboxsep=4pt\relax

%\newtheorem{proposition}{Proposition}
 
\usepackage{booktabs}
 % The siunitx package is used by this sample document
 % to align numbers in a column by their decimal point.
 % Remove the next line if you don't require it.
\usepackage[load-configurations=version-1]{siunitx} %newer version
 %\usepackage{siunitx}
\usepackage{tikz}
\usetikzlibrary{shapes,snakes}
\usetikzlibrary{bayesnetID}

\usepackage{bm}
\usepackage{arydshln}
\usepackage{tabularx}

\usepackage[draft]{minted}
\usemintedstyle{friendly}

\usepackage{fancyvrb}
\usetikzlibrary{external}

\graphicspath{{./img/}}

% avoid capitalizing LIST OF ALGORITHMS header
\let\MakeUppercase\relax

\sisetup{
round-mode = places,
round-precision = 3,
table-number-alignment=right,
table-text-alignment=right,
group-four-digits
}

\lstset{basicstyle=\scriptsize\ttfamily,breaklines=true,showstringspaces=false}


\newcommand{\NEG}[1]{\mbox{-}#1}
\newcommand{\dd}[1]{\text{d}#1}
\newcommand\xqed[1]{%
  \leavevmode\unskip\penalty9999 \hbox{}\nobreak\hfill
  \quad\hbox{#1}}
\newcommand\demo{\xqed{$\triangle$}}
%\usepackage[colorlinks=true,allcolors=black,breaklinks=true]{hyperref}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{setspace}
\usepackage{hyperref}
%\usepackage{subfig}
%\usepackage[labelformat=simple]{subcaption}
%\renewcommand\thesubfigure{\alph{subfigure}}
%\DeclareCaptionLabelFormat{subcaptionlabel}{\normalfont(\textbf{#2}\normalfont)}
%\captionsetup[subfigure]{labelformat=subcaptionlabel}



% For algos:
\algblockdefx[Train]{Train}{EndTrain}[1][]{\textbf{Training} #1}{\textbf{End Training}}
\algblockdefx[Preprocessing]{Preprocessing}{EndPreprocessing}[1][]{\textbf{Preprocessing} #1}{\textbf{End Preprocessing}}
\algblockdefx[Operation]{Operation}{EndOperation}[1][]{\textbf{Operation} #1}{\textbf{End Operation}}

% References and citations
\usepackage[capitalize]{cleveref}
\crefname{appcha}{Appendix}{Appendices}

\usepackage[backend=bibtex8, 
            style=authoryear, 
            %citestyle=numeric,
            citestyle=authoryear,
            %bibstyle=authoryear,
            maxbibnames=15,
            maxcitenames=2, 
            doi=false,
            %url=false,
            giveninits=true,
            hyperref]{biblatex}
            
%\usepackage{natbib}


\renewcommand*{\nameyeardelim}{\addcomma\space}
% space between bibliography entries
\setlength\bibitemsep{0.5\baselineskip}
% last name first in second and subsequent authors
\DeclareNameAlias{sortname}{last-first}

\DeclareBibliographyCategory{mypapers}

\addbibresource{./bib/phd2021}

\usepackage{phd}

% Debug.
%\overfullrule=10mm

% Remove red border from links
\hypersetup{colorlinks=false, pdfborder={0 0 0}}

%\numberwithin{equation}{section}
\numberwithin{figure}{chapter}
\numberwithin{table}{chapter}

\pagestyle{headings}

\title{Contributions to Large Scale Bayesian Inference and Adversarial Machine Learning \\
\vspace{0.3cm}
{\Large (Contribuciones a la Inferencia Bayesiana a Gran Escala y al \\ Aprendizaje Automático Adversario) }}
\author{Víctor Gallego Alcalá}
\newcommand{\director}{David Ríos Insua and David Gómez-Ullate Oteiza}

\renewcommand{\thealgorithm}{\arabic{chapter}.\arabic{algorithm}} 
%\renewcommand\theequation{\thechapter.\arabic{equation}}



\usepackage{chngcntr}
\counterwithin{equation}{chapter}
\numberwithin{equation}{chapter}

\AtBeginEnvironment{subappendices}{%
\chapter*{Appendix}
\addcontentsline{toc}{chapter}{Appendices}
\counterwithin{figure}{chapter}
\counterwithin{table}{chapter}
\counterwithin{algorithm}{chapter}
}

\DeclareCiteCommand{\citejournal}
  {\usebibmacro{prenote}}
  {\usebibmacro{citeindex}%
    \usebibmacro{journal}}
  {\multicitedelim}
  {\usebibmacro{postnote}}
  
  
\setcounter{tocdepth}{1}



\begin{document}

\titulo

\cleardoublepage
\begin{description}[labelwidth=\widthof{\textbf{Department:}}, leftmargin=!, labelsep=2em]
\item[Department:] Estadística e Investigación Operativa \\ Facultad de Ciencias Matemáticas \\ Universidad Complutense de Madrid (UCM) \\ Spain
\item[Title:] Contributions to Large Scale Bayesian Inference and Adversarial Machine Learning
\item[Author:] Víctor Gallego Alcalá
\item[Advisors:] David Ríos Insua and David Gómez-Ullate Oteiza
\item[Date:] June 2021
\iffalse
\item[Committee:] \mbox{} \\
\begin{itemize}[itemsep=1.8cm, labelsep=0.5em, leftmargin=-2em]
\item President: Aníbal Ramón Figueiras Vidal
\item Secretary: Daniel Hernández Lobato
\item Vocal 1: César Hervás Martínez
\item Vocal 2: María Amparo Alonso Betanzos
\item Vocal 3: David Ríos Insua
\item Substitute 1: Ana María González Marcos
\item Substitute 2: Pedro Antonio Gutiérrez
\end{itemize}
\fi
\end{description}


\chapter*{Abstract}

The field of machine learning (ML) has experienced a major boom in the past years, both in theoretical developments and application areas. However, the rampant adoption of ML methodologies has revealed that models are usually adopted to make decisions without taking into account the uncertainties in their predictions. More critically, they can be vulnerable to adversarial examples, strategic manipulations of the data with the goal of fooling those systems. For instance, in retailing, a model may predict very high expected sales for the next week, given a certain advertisement budget. However, the predicted variance may also be quite big, thus making the prediction almost useless depending on the risk tolerance of the company. Similarly, in the case of spam detection, an attacker may insert additional words in a given spam email to evade being classified as spam by making it to appear more legit.
Thus, we believe that developing ML systems that take into account predictive uncertainties and are robust against adversarial examples is a must for critical, real-world tasks. This thesis is a step towards achieving this goal. 

In Chapter 1, we start with a case study in retailing. We propose a robust implementation of the Nerlove–Arrow model using a Bayesian structural time series model to explain the relationship between advertising expenditures
of a country-wide fast-food franchise network with its weekly sales. Its Bayesian nature facilitates incorporating prior information reflecting the manager’s views, which can be updated with relevant data. However, this case study adopted classical Bayesian techniques, such as the Gibbs sampler. Nowadays, the ML landscape is pervaded with complex models, huge in the number of parameters. This is the realm of neural networks and this chapter also surveys current developments in this sub-field. In doing this, three challenges that constitute the core of this thesis are identified.

Chapter 2 is devoted to the first challenge. In it, we tackle the problem of scaling Bayesian inference to complex models and large data regimes. In the first part, we propose a unifying view of two different Bayesian inference algorithms, Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) and Stein Variational Gradient Descent (SVGD), leading to improved and efficient novel sampling schemes. In the second part, we develop a framework to boost the efficiency of Bayesian inference in probabilistic models by embedding a Markov chain sampler within a variational posterior approximation. We call this framework “variationally inferred sampling”. This framework has several benefits, such as its ease of implementation and the automatic tuning of sampler parameters, leading to a faster mixing time through automatic differentiation. Experiments show the superior performance of both developments compared to baselines. In addition, both could be combined to further improve the results.

In Chapter 3, we address the challenge of protecting ML classifiers from adversarial examples. So far, most approaches to adversarial classification have followed a classical game-theoretic framework. This requires unrealistic common knowledge conditions untenable in the security settings typical of the adversarial ML realm. After reviewing such approaches, we present an alternative perspective on AC based on adversarial risk analysis, and leveraging the scalable Bayesian approaches from chapter 2.

In Chapter 4, we turn our attention form supervised learning to reinforcement learning (RL), addressing the challenge of supporting an agent in a sequential decision making setting where there can be adversaries, specifically modelled as other players. We introduce Threatened Markov Decision Processes (TMDPs) as an extension of the classical Markov Decision Process framework for RL. We also propose a level-$k$ thinking scheme resulting in a novel learning approach to deal with TMDPs. After introducing our framework and deriving theoretical results, relevant empirical evidence is given via extensive experiments, showing the benefits of accounting for adversaries in RL while the agent learns.

Finally, Chapter 5 sums up with several conclusions and avenues for further work.
\vspace{1cm}

The following papers derived from this thesis have been published or already accepted:
\begin{itemize}
    \item \cite{gallego2019dlms}. \citetitle{gallego2019dlms}. In \citejournal{gallego2019dlms}.
    \item \cite{gallego2019reinforcement}. \citetitle{gallego2019reinforcement}. In \citejournal{gallego2019reinforcement}.
     \item \cite{gallego2019vis}. \citetitle{gallego2019vis}. In \citejournal{gallego2019vis}.   
     \item \cite{math8111957}. \citetitle{math8111957}. In \citejournal{math8111957}.
     \item \cite{nn2022}. \citetitle{nn2022}. In \citejournal{nn2022} (to appear).
\end{itemize}


The following papers derived from this thesis are under submission:
\begin{itemize}
    \item \citeauthor{gallego2019opponent}. \citetitle{gallego2019opponent}. 
        \item \citeauthor{gallego2021data}. \citetitle{gallego2021data}. 
    \item \citeauthor{gallego2018stochastic}. \citetitle{gallego2018stochastic}. 
    \item \citeauthor{AMLARA}. \citetitle{AMLARA}. 
     \item \citeauthor{gallego2020protecting}. \citetitle{gallego2020protecting}. 
\end{itemize}

The following papers related to the contents of this thesis were also published:
\begin{itemize}
    \item \cite{angulo2018bayesian}. \citetitle{angulo2018bayesian}. In \citejournal{angulo2018bayesian}.
    \item \cite{banks2020adversarial}. \citetitle{banks2020adversarial}. In \citejournal{banks2020adversarial}.
\end{itemize}

\chapter*{Resumen}

El campo del aprendizaje automático (AA) ha experimentado un auge espectacular en los últimos años, tanto en desarrollos teóricos como en áreas de aplicación. Sin embargo, la rápida adopción  de las metodologías del AA ha mostrado que los modelos que habitualmente se emplean para toma de decisiones no tienen en cuenta la incertidumbre en sus predicciones o, más crucialmente, pueden ser vulnerables a ejemplos adversarios, datos manipulados estratégicamente con el objetivo de engañar estos sistemas de AA. Por ejemplo, en el sector de la hostelería, un modelo puede predecir unas ventas esperadas muy altas para la semana que viene, fijado cierto plan de inversión en publicidad. Sin embargo, la varianza predictiva también puede ser muy grande, haciendo la predicción escasamente útil según el nivel de riesgo que el negocio pueda tolerar. O, en el caso de la detección de spam, un atacante puede introducir palabras adicionales en un correo de spam  para evadir el ser clasificado como tal y aparecer  legítimo.
Por tanto, creemos que desarrollar sistemas de AA que puedan tener en cuenta también las incertidumbres en las predicciones y ser más robustos frente a ejemplos adversarios es una necesidad para tareas críticas en el mundo real. Esta tesis es un paso hasta alcanzar este objetivo.

En el capítulo 1, empezamos con un caso de estudio en el sector de la hostelería. Proponemos una implementación robusta del modelo de Nerlove-Arrow usando un modelo  estructural bayesiano de series temporales para explicar la relación entre las inversiones en publicidad con las ventas semanales de una cadena nacional de restaurantes de comida rápida. Su naturaleza bayesiana facilita  incorporar conocimiento a priori que refleje las creencias del gestor, y pueden  actualizarse con datos observados. Sin embargo, este caso de estudio emplea técnicas bayesianas ya clásicas, como el muestreador de Gibbs. Hoy en día, el panorama del AA está repleto de modelos complejos, enormes en cuanto a número de parámetros. Este es caso de las redes neuronales, así que en este capítulo también resumimos los avances recientes en este subcampo. Tres desafíos constituyen el cuerpo de esta tesis.

El capítulo 2 va dedicado al primer desafío. En él, atacamos el problema de escalar la inferencia Bayesiana a modelos complejos o regímenes de grandes datos. En la primera parte, proponemos una visión unificadora de dos algoritmos de inferencia Bayesiana, Monte Carlo mediante cadenas de Markov con Gradientes Estocásticos y Descenso por el Gradiente Variacional Stein, llegando a  esquemas mejorados y eficientes de inferencio. En la segunda parte, desarrollamos una metodología para mejorar la eficiencia de la inferencia bayesiana mediante el anidamiento de un muestreador basado en cadenas de Markov dentro de una aproximación variacional. A esta metodología la llamamos "aproximación variacional refinada". La metodología conlleva varios beneficios, como su facilidad de implementación y el ajuste automático de los hiperparámetros del muestreador, logrando tiempos de convergencia más rápidos gracias a la diferenciación automática. Los experimentos muestran el rendimiento superior de ambos desarollos comparado con algunas alternativas.

En el capítulo 3, nos centramos en el desafío de proteger clasificadores de AA de los ejemplos adversarios. Hasta ahora, la mayoría de enfoques en clasificación adversaria han seguido el paradigma clásico de teoría de juegos. Esto requiere condiciones poco realistas de conocimiento común, que no son admisibles en entornos típicos en seguridad del aprendizaje automático adversario. Tras revisar estos enfoques, presentamos una perspectiva alternativa basada en análisis de riesgos adversarios y aprovechamos las técnicas bayesianas escalables del capítulo 3.

En el capítulo 4, pasamos nuestra atención del aprendizaje supervisado al aprendizaje por refuerzo (AR), incidiendo en el desafío de apoyar un agente en un escenario de toma de decisiones secuenciales en el que puede haber adversarios, modelizados como otros jugadores. Introducimos los Procesos de Decisión de Markov Amenazados como una extensión del paradigma clásico de los procesos de decisión Markovianos. También proponemos un esquema basado en pensamiento de nivel-$k$ resultando en un nuevo algoritmo de aprendizaje. Tras introducir la metodología y derivar algunos resultados teóricos, damos evidencia empírica relevante mediante experimentos extensos, mostrando los beneficios de modelizar oponentes en AR mientras el agente aprende.

Finalmente, en el capítulo 5 terminamos con varias conclusiones y posibles extensiones para trabajo futuro.

\vspace{1cm}
Los siguientes artículos derivados de esta tesis ya han sido publicados o están aceptados:
\begin{itemize}
    \item \cite{gallego2019dlms}. \citetitle{gallego2019dlms}. En \citejournal{gallego2019dlms}.
    \item \cite{gallego2019reinforcement}. \citetitle{gallego2019reinforcement}. En \citejournal{gallego2019reinforcement}.
     \item \cite{gallego2019vis}. \citetitle{gallego2019vis}. En \citejournal{gallego2019vis}.   
     \item \cite{math8111957}. \citetitle{math8111957}. En \citejournal{math8111957}.
     \item \cite{nn2022}. \citetitle{nn2022}. En \citejournal{nn2022} (por aparecer).
\end{itemize}

Los siguientes artículos derivados de esta tesis se encuentran bajo revisión:
\begin{itemize}
    \item \citeauthor{gallego2019opponent}. \citetitle{gallego2019opponent}. 
        \item \citeauthor{gallego2021data}. \citetitle{gallego2021data}. 
    \item \citeauthor{gallego2018stochastic}. \citetitle{gallego2018stochastic}. 
    \item \citeauthor{AMLARA}. \citetitle{AMLARA}.
     \item \citeauthor{gallego2020protecting}. \citetitle{gallego2020protecting}. 
\end{itemize}


Los siguientes artículos relacionados con el contenido de la tesis también han sido publicados:
\begin{itemize}
    \item \cite{angulo2018bayesian}. \citetitle{angulo2018bayesian}. En \citejournal{angulo2018bayesian}.
    \item \cite{banks2020adversarial}. \citetitle{banks2020adversarial}. En \citejournal{banks2020adversarial}.
\end{itemize}

\addcontentsline{toc}{chapter}{Abstract}
\addcontentsline{toc}{chapter}{Resumen}

\chapter*{Agradecimientos}

Estos cuatro años de tesis se han pasado volando. Aunque se hayan materializado en parte en este documento, no puedo olvidarme de todas las personas que, de un modo u otro, han contribuido a este trabajo. \\

En primer lugar, debo agradecer enormemente la labor y apoyo de mis directores durante este período. A David Ríos, especialmente por su incansable atención y paciencia, sobre todo al leer mis textos y dudas; y a David Gómez-Ullate, por darme la oportunidad de empezar en este mundo de los datos hace ya cinco años. Gracias a los dos por todas las oportunidades. Les considero verdaderos mentores de los que he podido aprender muchísimo en estos años, no solo acerca de los temas de esta tesis, así que espero seguir manteniendo nuestra relación en el futuro. 
Asimismo, debo agradecer la labor del Prof. David Banks, quien me dio la oportunidad de hacer una estancia de investigación en Duke y SAMSI, resultando en una experiencia muy enriquecedora académica y vitalmente. También agradezco al Ministerio por la beca FPU16-05034, la cátedra AXA-ICMAT, el programa “Severo Ochoa”
para Centros de Excelencia en I+D, y la Fundación BBVA, entre otros. \\

También quería mencionar a los compañeros y amigos hechos en el grupo formado en el Datalab del ICMAT y el vecino IFT (y algunos por extensión ya, en Komorebi). Roi, David, Alberto R., Alberto T., Simón, Jorge, Bruno, April, Aitor, Alex, Nadir, Christian... 
Lamento que por la pandemia apenas nos veamos ya por el campus, pero siempre me acordaré de tantos buenos momentos. También quiero mostrar mi agradecimiento a los investigadores Pablo Angulo y Pablo Suárez, por haber tenido el placer de trabajar con ellos al principio de mi carrera. Y a Marta Sanz, por estar siempre dispuesta a resolver cualquier trámite. \\

Por último, mención especial merecen mis padres, Sotero y María, siempre dándome su apoyo y cuidado incondicional a pesar de todo. E Irene, por acompañarme siempre ahí y quien me hizo ver que todo es posible, con toda su ilusión, ingenio y magia. Gracias, os quiero.

\addcontentsline{toc}{chapter}{Agradecimientos}

\indice
\indicetablas
\indicefiguras
\indicealgoritmos

%\chapter*{Notation}
%TODO

\chapter{Introduction}\label{cha:intro}
% if put before \chapter{Introduction} Notation chapter has arabic pagenumbers
\setcounter{page}{1}
\pagenumbering{arabic}
\input{chapters/00_intro}

\begin{subappendices}
\input{chapters/appendix_dlm}
\end{subappendices}



\chapter{Large Scale Bayesian Inference}\label{cha:bayes}
\input{chapters/03_bayes}

\begin{subappendices}
\input{chapters/appendix_bayes}
\end{subappendices}


\chapter{Adversarial Classification}\label{cha:adv}
\input{chapters/04_adv}


\chapter{Issues in Multi-agent Reinforcement Learning}\label{cha:ararl}
\input{chapters/05_ararl}


\begin{subappendices}
\input{chapters/appendix_marl}
\end{subappendices}


\chapter{Conclusions}\label{cha:conclusions}
\input{chapters/08_conclusions}



\phantomsection
\addcontentsline{toc}{chapter}{\bibname}
\newrefcontext[sorting=nty]

\printbibliography

\end{document}
