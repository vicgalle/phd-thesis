
\section{A motivation for Bayesian methods}

Statistical decision theory \parencite{french} is a fundamental block of modern machine learning and statistics research and practice,
providing fundamental tools for analyzing the vast amount of data that have become available in science, government, industry, and everyday life. Over the last century, many problems have been solved (at least partially) with probabilistic models \parencite{bishop2006pattern}, such as classifying email as spam, identifying patterns in genetic sequences, recommending similar movies or performing automatic translation between two languages. For each of these applications, a statistical model was fitted to the data, typically solving the task at hand. Since the previous applications are incredibly diverse,  models came from  different research communities. But there was a common theme shared amongst them: \emph{the need to support a decision using the model}. Thus, statistical decision theory  emerged as a powerful framework to reason between different scientific areas. 

We believe that building and using probabilistic models is not a single-step task, but rather an iterative process, closely resembling the scientific method \parencite{conant1959understanding}. First, propose a simple model based on the latent structure that your prior knowledge make you believe exists in the data. Then, given a data set, use an inference
method to approximate the posterior distribution, the conditional distribution of the  parameters
given the data. Lastly,
use that posterior to test the model against new, test data. If not satisfied, we criticise and revise it, and we modify the model, iterating all over again. This is called the Box's loop \parencite{doi:10.1080/00401706.1962.10490015,10.2307/1266125}. It focuses on the scientific method, producing new knowledge  through iterative experimental design, data collection,
model formulation, and model criticism.
Amongst all the different paradigms in statistics, the one that is closest to the previous approach to knowledge discovery is Bayesian analysis \parencite{gelman2013bayesian,insua2012bayesian}, and this will be the one adopted in this thesis.


In the past years, we have witnessed the success of neural networks and deep learning, achieving state of the art results in many different tasks \parencite{lecun2015deep}. A brief introduction to deep learning will appear in Section \ref{sec:deep_intro}, but let us advance a few things here. Bayesian analysis is specially compelling for this kind of models, mostly because the object of interest  is now the predictive distribution,
$$
p(y|x, \mathcal{D}) = \int p(y|x,\theta) p (\theta| \mathcal{D}) d\theta,
$$
with $x$ being a sample to be classified or regressed, $y$ the predicted target, $\mathcal{D}$ the training dataset, and $\theta$ the parameters of a neural network. The previous marginalization equation expresses epistemic uncertainty, that is, uncertainty over which configuration of parameters (hypothesis) is correct, given limited data. If the posterior $p(\theta| \mathcal{D})$ is sharply peaked, it makes no difference using a Bayesian approach versus the standard maximum likelihood or maximum a posteriori estimations, since just a single point mass may be a reasonable approximation to the posterior. However, neural networks are typically very underspecified by the training data available (a state of the art NN might have millions of parameters for just thousands of data points), and will thus have diffuse likelihood distributions $p(\mathcal{D}|\theta)$, leading to flatter posteriors which are also multi-modal.

Indeed, there are large valleys in the loss landscape of
neural networks \parencite{garipov2018loss}, over which parameters incur very little loss, but give rise to different high performing
functions which make meaningfully different predictions on the test dataset. The work of \parencite{ZOLNA2020102969}
also demonstrates the variety of well-performing solutions that can be expressed by a neural network
posterior, as it is highly flexible. In these cases, we desire to perform  Bayesian model averaging, since it leads to an ensemble of diverse yet good models, achieving better generalization  and performance stability capabilities than classical training.

In this thesis, we provide several contributions to large scale Bayesian machine learning, motivated by the following case study. On the one hand, it serves us to introduce notation and key concepts, and, on the other hand, the case will motivate our three main problems of interest.

\section{A case study in retailing}\label{sec:dlms}
\input{chapters/002_dlm}

\section{State of the art models and big data}\label{sec:deep_intro}
\input{chapters/003_deep}

\section{Challenges}

The reflections motivated by the case study in Section 1.2, using classical Bayesian approaches, and the review in Section 1.3, have led us to enumerate the following three major challenges, object of study in this thesis.

\subsection{Challenge 1. Large scale Bayesian inference}

%\subsubsection{Bayesian approaches}

MCMC algorithms such as the ones presented in Section \ref{s:MCMC} or \ref{bayeshallow} 
have become standard in Bayesian inference \parencite{french}.
% Moreover, more recent variants such as HMC
% and its Riemann
%manifold variant, gain 
%some efficiency of exploration,
However, they entail a significant computational burden in large datasets. Indeed, 
computing the corresponding acceptance probabilities 
 requires iterating over the whole dataset, which often does not even
 fit into memory. Thus, they 
  do not scale well in big data settings. 
 As a consequence, several approximations have been proposed.

%%%%%%%%%%%%%%%%%%%%
\paragraph{Stochastic Gradient Markov chain Monte Carlo.}\label{bayesdeep} 

SG-MCMC methods are based on the discretization of 
stochastic differential equations that have the desired target 
distribution as its limit. \textcite{ma2015complete} provide a
complete framework that encompass many earlier proposals and
facilitate such discretization, as well as a practical tool for
devising new samplers and testing the correctness of proposed samplers.
We aim at drawing samples from the
posterior $p(\theta |D) \propto \exp(-U(\theta ))$,
with potential function
$U(\theta ) = -\sum _{x\in D} \log p(x|\theta ) + \log p(\theta )$. Define also auxiliary variables $r$,
with $z=( \theta, r )$, and sample from $p(z|D) \propto  \exp(-H(z))$, with hamiltonian
$H(z) = H(\theta , r) = U(\theta ) + g(\theta , r)$, such that
$\exp(-g(\theta , r))dr = constant$. 
Marginalizing the auxiliary variables gives us the desired distribution on $\theta $.

In general, all continuous Markov processes that one might consider for sampling can be written
as a stochastic differential equation (SDE) of the form:
\begin{equation}
dz = f(z)dt +
\sqrt{ 2D(z)}dW(t),
\end{equation}
where $f(z)$ denotes the deterministic drift, frequently
related to $\nabla H(z)$,
$W(t)$  is a $d$-dimensional Wiener process, and 
$D(z)$ is a positive semidefinite diffusion matrix. Note, though,
that some care must be taken to choosing $f(z)$ and $D(z)$ 
to yield the desired stationary distribution.
\parencite{ma2015complete} propose a recipe for constructing SDEs with the correct stationary distribution through 
$f(z) = - [D(z) + Q(z)] \nabla H(z) + \Gamma (z)$, 
and $\Gamma _i (z) = \sum _{j=1}^d 
\frac{\partial  }{\partial z_j}(D_{ij} (z) + Q_{ij} (z) )
$
where $ Q(z)$ is a skew-symmetric curl matrix (representing the deterministic traversing effects seen
in HMC procedures) and the diffusion matrix $D(z)$
determines the strength of the Wiener process-driven diffusion.
When 
$D(z)$ is positive semidefinite and $Q(z)$ is skew-symmetric, 
 the convergence of the above dynamics to the desired 
distribution follows; moreover both matrices can be adjusted to attain faster convergence to
the posterior distribution. \parencite{ma2015complete} show that by properly choosing the matrices  one can recover numerous samplers such
as SGLD \parencite{welling2011bayesian} or a corrected SG-HMC \parencite{chen2014stochastic}.

In practice, simulation actually relies on a discretization of the SDE, leading to a (full-data) update rule
\begin{equation}
z_{t+1} \leftarrow z_t - \epsilon_t \left[ ( D(z_t) + Q(z_t) )
\nabla H(z_t) + \Gamma (z_t)\right]
+ N (0, 2\epsilon _t D(z_t)).
\end{equation} 
Calculating $\nabla H(z)$ entails evaluating the gradient of $U(\theta )$
which, with deep NN models, 
 becomes very intensive computationally as it relies on a sum
over all data points. Instead, 
we use a sampled data subset $S' \subset S$, with the corresponding potential for these data being 
$U_1 (\theta ) = -\frac{|S'|}{|S|} \sum _{x \in S'}
\log p(x|\theta ) + \log p(\theta )$, leading 
to the approximation 
\begin{equation}
z_{t+1} \leftarrow z_t - \epsilon_t \left[ ( D(z_t) + Q(z_t) )
\nabla H (z_t) + \Gamma (z_t) \right]
+ N (0, \epsilon _t (2 D(z_t)- \epsilon_t \hat {B_t}))
\end{equation} 
where $\hat {B_t}$ is an estimate of the variance of the error.
This provides the  stochastic gradient—or minibatch— variant of the sampler. Note also that all of these approaches could be combined with recent heuristics to improve the convergence of SG-MCMC methods, such as the adoption of cyclical step sizes to explore more efficiently the posterior  \parencite{7926641}.


%%%%%%%%%%%%%%%%%%%%%
\paragraph{Variational Bayes.} 

Variational inference (VI) \parencite{blei2017variational} tackles the 
 approximation of  $p(\theta | D)$ with a tractable parameterized
 distribution $q_{\phi}(\theta |D)$. The goal is to find parameters $\phi$ so that the distribution 
$q_{\phi}(\theta |D )$  (referred to as variational guide
or variational approximation)  is as close as possible to the actual posterior, with closeness typically measured through 
the Kullback-Leibler 
divergence $KL(q_{\phi } || p)$, reformulated into the ELBO
\begin{equation}\label{eq:elbo}
\mbox{ELBO}(q) = \mathbb{E}_{q_{\phi}(\theta |D)} \left[ \log p(D,\theta ) - \log q_{\phi}(\theta |D)\right],
\end{equation}
the objective to be optimized,
usually through SGD techniques. 

A standard choice
for $q_{\phi}(\theta |D )$ is a factorized Gaussian 
distribution $\mathcal{N}(\mu_{\phi}(D), \sigma_{\phi}(D))$,
with  mean and covariance matrix defined through a
 deep NN conditioned on the observed data $D$.
 Note though that 
other distributions can be adopted as long as they 
 are easily sampled and their log-density and entropy evaluated. 
A problem is that these approximations often 
underestimate the uncertainty. Some developments
partly mitigate this  issue
by  enriching the variational family include normalizing flows \parencite{rezende2015variational} or the use of implicit distributions \parencite{huszar2017variational}.

\iffalse
We can actually use a more flexible approximation \parencite{VIS},
 designated {\em refined variational approximation}, 
  by embedding a sampler (Section 3.3.1) through
\begin{equation*}\label{eq:q}
q_{\phi,\eta}(\theta |D) = \int Q_{\eta, T}(\theta  | \theta _0)q_{0,\phi}(\theta _0|D)d\theta _0,
\end{equation*}
where $q_{0,\phi} (\theta | D)$ is the initial and tractable density
$q_{\phi} (\theta | D)$.  
The conditional distribution $Q_{\eta, T}(\theta |\theta _0)$
designates a stochastic process parameterized by $\eta$,  
used to evolve the original density $q_{0,\phi}(\theta |D )$
for $T$ periods, so as to achieve greater flexibility. 
 Observe that when $T=0$, no refinement steps are performed and the refined variational approximation coincides with the original one; on the other hand, as 
 $T$ increases, the approximation will be closer to the exact posterior, assuming that $Q_{\eta, T}$ is a valid sampler
 as in Section \ref{bayesdeep}. Specific 
forms for $Q_{\eta, T}(\theta |\theta _0)$
are described in \parencite{VIS}.
We next maximize a refined ELBO objective, replacing in (\ref{eq:elbo}) the 
original $q_{\phi }$ 
by $q_{\phi, \eta}$
\begin{equation}\label{eq:ELBO}
\mbox{ELBO}(q_{\phi,\eta}) = \mathbb{E}_{q_{\phi, \eta}(\theta D)}
\left[ \log p(D, \theta ) - \log q_{\phi, \eta}(\theta |D)\right]
\end{equation}
to optimize the divergence $KL(q_{\phi,\eta}(\theta |D) ||  p(\theta |D ))$. 
%{\bf{The first term of Eq. (\ref{eq:ELBO})}}
%requires only being able to sample from $q_{\phi,\eta}(z|x)$; however the second
%one, the entropy
%$-\mathbb{E}_{q_{\phi,\eta}(z|x)} \left[ \log q_{\phi,\eta}(z | x) \right]$, requires also evaluating the evolving, implicit density.
%Section 3.2 describes 
Efficient methods to approximate 
 such 
evaluation are available in \parencite{VIS}. 

As a consequence, performing variational inference with the refined variational approximation can be regarded as using the original variational guide while optimizing an alternative, tighter ELBO.  
This facilitates a framework for learning the sampler parameters $\phi, \eta$ using gradient-based optimization based 
on two phases: first,
 refinement, sampler parameters are learnt in an optimization loop that maximizes the ELBO with the new posterior;
after several iterations, the inference  phase 
starts and we just let the tuned sampler run for
sufficient iterations as used in SG-MCMC samplers. %Algorithmically, this is expressed as in Algorithm \ref{alg:vis}.
\iffalse
{\tt 
\begin{description}
    \item[Refinement phase] Repeat until convergence:
    \begin{enumerate}
    \item Sample initial set of particles, $\theta _0 \sim q_{0,\phi}(\theta | D)$.
    \item Refine particles through sampler, $\theta _T \sim Q_{\eta, T}(\theta |\theta 0)$.
    \item Compute the ELBO objective (\ref{eq:ELBO}). 
    \item Update parameters $\phi, \eta$ through automatic 
    differentiation on objective.
\end{enumerate}
    \item[Inference phase] Based on learnt sampler parameters $\phi^*, \eta^*$:
    \begin{enumerate}
    \item Sample an initial set of particles, $\theta _0 \sim q_{0,\phi^*}(\theta |D)$.
    \item Use the MCMC sampler $\theta _T \sim Q_{\eta^*, T}(\theta |\theta _0)$ as $T \rightarrow \infty$.
    \end{enumerate}
\end{description}
}

{\tt
\begin{algorithm}[H]
Refinement phase: \\
\While{not convergence}{
 Sample initial set of particles, $\theta _0 \sim q_{0,\phi}(\theta | D)$. \\
 Refine particles through sampler, $\theta _T \sim Q_{\eta, T}(\theta |\theta 0)$. \\
 Compute the ELBO objective (\ref{eq:ELBO}).  \\
 Update parameters $\phi, \eta$ through automatic 
    differentiation on objective. \\
 }

Inference phase. Based on learnt sampler parameters $\phi^*, \eta^*$:\\
\quad Sample an initial set of particles, $\theta _0 \sim q_{0,\phi^*}(\theta |D)$. \\
\quad  Use the MCMC sampler $\theta _T \sim Q_{\eta^*, T}(\theta |\theta _0)$ as $T \rightarrow \infty$.
 \caption{VIS sampler}\label{alg:vis}
\end{algorithm}
}
\fi

\fi
%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
\paragraph{Ensembles.} \parencite{bdl} showed that deep ensembles \parencite{ensemble} provide an efficient technique for approximate Bayesian marginalization, although it is not a truly Bayesian method.

A regularization technique called dropout can be interpreted as performing ensembling over several models \parencite{dropout}. Motivated by how dropout could be effectively used to quantify uncertainty in deep networks \parencite{pmlr-v48-gal16}, several explorations have been pursued to extend ensembles for similar purposes. Deep ensembles were found to be superior to MC dropout in out-of-distribution-settings \parencite{Ovadia2019CanYT}.
Some recent techniques that should be pointed out include
Stochastic Weight Averaging (SWA) \parencite{izmailov2018averaging}, which leads to much more robust optima and better generalization, and a Bayesian treatment of the previous approach based on a Gaussian distribution,
called SWAG \parencite{NEURIPS2019_118921ef}.

Stein Variational Gradient Descent (SVGD) \parencite{svgd} takes a set of particles and evolves it following the gradient of the loss plus an auxiliary term that acts as a repulsion force between particles. That
way, the particles do not collapse into the same mode, and can explore wider areas of the posterior. There is a close relationship between SVGD and SG-MCMC methods with \parencite{gallego2018stochastic} 
studying the interaction between both methods.

Deep learning ensemble applications abound, for example in probabilistic wind power forecasting \parencite{wang2017deep}, speech recognition \parencite{deng2014ensemble}, time series forecasting \parencite{qiu2014ensemble}, and medicine or bioinformatics \parencite{qummar2019deep,xiao2018deep,cao2020ensemble}. However, the vast majority of these applied works only use simple ensembles, by averaging the outputs. Further benefits could be achieved by realizing a full Bayesian treatment of the problem.
\fi

In Chapter 2 we will further review these approaches and propose two frameworks to further scale up Bayesian inference in challenging settings. The first is based on augmenting an SG-MCMC sampler with multiple, parallel chains, incorporating interactions between particles so they can explore the posterior more efficiently. The second is a novel variational approximation that serves to automatically adapt the hyperparameters of any SG-MCMC sampler.
%\paragraph{Final remarks}
%Lastly, note that 


%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection{Challenge 2. Security of machine learning}
\label{sec:chall2}

As described, over the last decade
an increasing number of processes are being automated through 
deep NN algorithms, being 
essential that these are robust and reliable
if we are to
trust operations based on their output. State-of-the-art
algorithms, as those described above, perform extraordinarily well on standard data,  but have been
shown to be vulnerable to adversarial examples, data instances targeted at
fooling them \parencite{goodfellow2014explaining}.
The presence of adaptive adversaries has
been pointed out in areas such as spam detection \parencite{zeager2017adversarial}
and computer vision \parencite{goodfellow2014explaining}, among many others. 
In those contexts, algorithms should acknowledge the presence of possible adversaries
to protect from their data manipulations.
As a fundamental underlying hypothesis, NN
based systems rely on using 
independent and identically distributed (iid) data for both training and operations. However, security aspects in deep
learning, part of the emergent field of
adversarial machine learning (AML),
question such hypothesis, given the
presence of adaptive adversaries ready to  intervene in the problem 
to modify the data and obtain a benefit. In addition, time series models, such as the introduced in Section \ref{sec:dlms} can also be subject to adversarial attacks \parencite{9063523,10.5555/3016100.3016102}, so it is of great interest to develop model-agnostic defences.

As a motivating example, vision algorithms (Section
\ref{kkvision}) are at the core of many AI 
applications such as autonomous driving systems (ADSs) \parencite{rumanos}. 
The simplest and most notorious attack examples to
such algorithms  
consist of modifications of images in such a way that the alteration becomes insignificant to the human eye, yet drives a model trained on millions of images to misclassify the modified ones,
with potentially relevant security consequences.
With a relatively simple CNN model, we are able to accurately predict 99\% of the handwritten digits in the MNIST data set.
However, if we attack those data with the fast gradient sign method \parencite{szegedy2013intriguin},
accuracy gets reduced to 62\%. Fig.~\ref{fig:digits} provides an example of an original MNIST image and a perturbed one: to our eyes both images look like a 2, but the classifier rightly identifies a 2 in the first case, whereas it suggests a 7 after the perturbation. 
%
%\vspace{-0.2in}
%
%\hfill $\triangle$
%
\begin{figure}[hbt]
\centering
  \includegraphics[width=.6\linewidth]{figures/27}
  \caption{Left: original image, correctly classified as a 2. Right: slightly perturbed image, wrongly classified as a 7.}
  \label{fig:digits}
\end{figure}
Stemming from the pioneering work in adversarial classification 
\parencite{dalvi2004adversarial}, the prevailing paradigm in AML models
the confrontation between learning-based systems and adversaries through game theory. 
This entails common knowledge assumptions 
\parencite{hargreaves2004game} which are 
questionable in security 
applications as adversaries try to conceal information. 
As \parencite{fan2019selective} points out, there is a need for a  framework that guarantees robustness of ML against adversarial manipulations in a principled manner. 

The usual approach for robustifying models against these examples is {\em adversarial training} (AT) \parencite{madry2018towards} and its 
variants, based on solving a 
bi-level optimisation problem whose objective function is the empirical risk of a model under worst case data perturbations. % AT approximates the inner optimisation through a projected gradient descent  
%algorithm, ensuring that the perturbed input falls within a tolerable boundary, usually specified through some restriction on a norm distance. 
However, recent pointers urge modellers to depart from using 
norm based approaches \parencite{carlini2019evaluating} and develop more realistic attack models.

AML is a  difficult area which rapidly evolves and leads to an 
arms race in which the community alternates cycles of proposing attacks and  of implementing defences that deal with them. However, as mentioned, it is 
based on game theoretic ideas and strong common
knowledge conditions. The challenge is thus to develop defence mechanisms that are sufficiently scalable to the increasingly complex ML models using in real life.
Chapter 3 examines these issues, providing a scalable defence procedure inspired by ARA \parencite{rios2009adversarial,banks2015adversarial}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\subsection{Challenge 3. Large scale competitive decision making}

In real life scenarios, decision makers rarely have to take a single action. Instead, multiple decisions must be made, with the former actions making a causal impact on the later ones. For example, in the case study from Section \ref{sec:dlms}, the decision maker might desire to plan ahead for a year the budget allocations of all the advertising channels. Thus, we have to delve into the realm of sequential decision making \parencite{french,DIEDERICH200113917}. There are many paradigms to tackle this problem, such as optimal control theory \parencite{kirk2004optimal}. Since this thesis is also focused on Machine Learning, we adopt the framework of Reinforcement Learning (RL) to deal with sequential decisions while making the decision maker able to learn from her experiences \parencite{sutton2012reinforcement,kaelbling1996reinforcement}. RL differs from supervised learning in not needing labelled input/output pairs be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted experiences) and exploitation (of current knowledge). Of course, RL can also benefit from recent developments in deep, neural architectures to further enhance its efficiency in complex and highly dimensional environments \parencite{mnih2015human}.

However, RL typically only deals with one agent (the decision maker), taking actions against her environment, which is assumed to be stationary. Realistic settings have to take into account the presence of other rational agents. These can be potential collaborators to cooperate, but also could be adversaries. For instance, in the case study from Section \ref{sec:dlms} we could have modeled competitors also taking decisions that in a way, also affect the expected sales of the supported franchise. Under this dynamic, non-stationary environment, traditional RL techniques dramatically fail, and it is a necessity to develop novel frameworks that acknowledge the presence of other players. This is the field of multi-agent Reinforcement Learning (MARL), which usually takes grounding in game-theoretical approaches \parencite{marl_over,lanctot2017unified}. 

Game Theory has also its own drawbacks, whereas Adversarial Risk Analysis (ARA) offers a more realistic view, leveraging Bayesian ideas \parencite{Banks}. The challenge is thus how to adapt the ARA methodology into the sequential learning nature of RL, with low computational requirements in order to allow for scalability.
Chapter 4 deals with some of these issues.


\section{Thesis structure}

The three challenges explained above are the content of the following chapters. Chapter 2 describes two novel approaches for large scale Bayesian inference in complex models. Chapter 3 surveys the state of adversarial classification and presents an original approach to robustifying classifiers inspired by Adversarial Risk Analysis. Chapter 4 takes a deep dive into reinforcement learning, proposing a framework to support a decision maker against adversaries and then studying a realistic application in data sharing markets.

Finally, Chapter 5 ends up with several conclusions and avenues for further work. Figure \ref{fig:thesis} shows how this thesis is structured.


\begin{figure}[h]
\centering
\includegraphics[scale=0.8]{figures/thesis_structure.png}
\caption{The structure of this thesis}\label{fig:thesis}
\end{figure}

